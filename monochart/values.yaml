replicaCount: 1

serviceAccount:
  create: false
  annotations:
  labels:
  name: 

dockercfg:
  enabled: false
  # image:
  #   pullSecret:
  #     registry: r.cfcr.io
  #     username: example
  #     password: password

image:
  repository: scratch
  tag: latest
  pullPolicy: IfNotPresent
  ## Additional docker pull secrets
  # pullSecrets:
  #   - "docker-secret-1"
  #   - "docker-secret-2"

configMaps: {}
  # default:
  #   enabled: false
  #   mountPath: /config-default
  #   annotations:
  #     name: value
  #   labels:
  #     name: value
  #   env:
  #     ENV_NAME: ENV_VALUE
  #   files:
  #     "test.txt": |-
  #         ...
  #     "test.yaml":
  #         group:
  #           key: value

secrets: {}
  # default:
  #   enabled: false
  #   mountPath: /secret-default
  #   annotations:
  #     name: value
  #   labels:
  #     name: value
  #   env:
  #     ENV_NAME: ENV_VALUE
  #   files:
  #     "test.crt": |-
  #         ...
  #     "test.yaml":
  #         group:
  #           key: value



# SealedSecrets resouces
# Basically we need to add a name to the resource and all the secrets/values that we want.
# the secret name can be changed without problem, but the resource name or the value *NO*
# to create a sealedsecrets you can use the webseal or https://webseal.qa.spoton.sh/
# NOTE: remember to add the sealedsecret name to the "envFrom.secrets" above.
sealedsecrets: 
  enabled: false
  # keys:
  #   - name: google-secrets
  #     encryptedData:
  #       SECRET_NAME: AgSDSDSB7oTOgJdtURoUrZ7IDuC50xdfdEMWFwAeashNXnM+s+QcdAXc2Bp99uD2RKYjCvdkP/+OzLmF0SqVfqjPM/+toplTMgNSDQSXGKZMQmrn+fXeq61qXqj2vQ/2tz/98DYXDQXJK3LPpCT/bXJ0Zv6jjxBkONOqkko//PWDxDk/zqVM75MH5qhy8K5Ds9YpQ3aWCtHuRXe1365fQPDpl+hJGZZpC1NZbQFUWpNrXOLQrMRXP8fOZ3+VWo0+elebPE9OpxoWuYXQHX4Dn8B7ArNp6XPJbbxBgsKNv9pG7VHNDRH98kUbttfOMJjIYF0d2IoiAPA9mwSsY3S7Jn0/dp8PL9WgNiILaVDWihs9nhg/iJZvPD8mymNZOggQH8gGkMQQ8p+rgWKeenTwHpC0gfZjQ73OyLuSKhYHn0tvmloiDq0dDreoZCiahZEfZ3MGaOD7xqj8qHmsHxzaDlilBLiM0X1XOmSVxs1Nvmka6vAQEzX6fesyAmPfKqdPEPGpjn39lkWKOHJqby3UnAyjNeTkCvjc4hwQwzNmjivyrbskQGwj3XxpVZruD9XHDRgMca/r3T9NVIpng0yVEmA6HKVOLupUMHvHsY4rE+2n5du/NmBaaw3lM5MybnhxIDQGQz8bFhxr5iIM9vBhF+zRz0ahtAULODh0+ebVHrCbQQnK3EzrkQzxqu6EPPIsuPQepcdeCAmNnoS0/JC2xC1rmEz
  #       ANOTHER_SECRET: AgCSDSDSNXUuenZ8asasFeVEOG+BTH9AdfdfdzbqYcz9VK2GIJdXgnInUv/N7wnIz77yjMd52qXV9SuNJo9RIJwJ+ioo0PuTYBT2ueApCras6hRMtey1fx0shjNJZjYv6uVWsmdd+Y17BTeeB7J6wo0spv/GfnHrtrTn2AfMxV/M2brSxG4yQ6gsrUNEdhdHcII47ae7DDqjEJ2UZgVdRCv8o8AKCu/NqiPEJklLvcXUigKc7KNyGVoJX9uFWGl4buu7hs5IpKfj58tbi6+8yW5YW3PyVbkydIxVAZrGRZS5G60jbCc7UGmo3fuWI7YBZtbsWw8alBexOX/U6+CDMRGClqdick5GGM0cGXHfsSM1XTMQBMxuWTzvEO2RnrgiRZ57PcBh5xrvBRQu49LhZAukPvLj2x6P2jrhU7xrjO+YI1kSZow7tuiIQNZuR+CKNxpZuNUqYvci3jRbj3Pxg0W9AkntTouFxyd4O02yz+OP25Zfqa+F1Uu/JZ9Zkph1/vykixCRc3sEx2oDXjQKFoNFnbCfi7vhNsZNfnLc/jOhXoMOrd63ZR+37cAXyiZ/6GxzWSYygb1CaR9PtALWvkThbgEYRGWdMVKVqyk6Br7K8gyodMNx1/QAX0/Qd9ykAMVoFhC6ykrVvkdPG2n65Ftx0jLrPQ3U75bijffl7e0UvA4olhBxtmgSMymAmKrCs59EqXhAfBcOdaUOwSguyZAJuJ3Wza9eJWFQ==

  #   - name: aws-s3
  #     encryptedData:
  #       USER_ADMIN: sfhiewhiewnfilblfiabflisafblisafalifaifnakfs....
  #       PASS_ADMIN: dfasifnaisnaisdnbafsbasfbsaif...

# Inline ENV variables
env: {}
  # ENV_NAME: ENV_VALUE

# ENV variables from existing Kubernetes secrets and ConfigMaps
#envFrom:
#  secrets:
#    - secret-1
#    - secret-2
#  configMaps:
#    - config-1
#    - config-2

# ENV variables from fieldRef.fieldPath
# https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#use-pod-fields-as-values-for-environment-variables
#envFromFieldRefFieldPath:
#  ENV_1: path-1
#  ENV_2: path-2

# ENV variables from  secretkeyref
# https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables
# envFromSecretKeyRef:
#   - name: MY_ENV_VARIABLE
#     secret: kubernetes-secret-name
#     key: key-name-in-secret


initcontainers:
  enabled: false

  FirstInitContainer: |
    - name: do-something
      image: busybox
      command: ['sh', 'sleep 60']

  extraInitContainer: {}

deployment:
  enabled: true
  ## Pods replace strategy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  # strategy: {}
  revisionHistoryLimit: 10
  # annotations:
  #   name: value
  # labels:
  #   name: value



  pod:
    command: []
    # hostAliases:
    #  - ip: "10.1.2.3"
    #    hostnames:
    #    - "foo.remote"
    #    - "bar.remote"
    annotations: {}
    ## https://github.com/uswitch/kiam
    ## https://github.com/jtblin/kube2iam
    #  iam.amazonaws.com/role: role-arn
    labels: {}
    # command:
    args: []
    lifecycle: {}


FirstVolumeMounts: {}
# FirstVolumeMounts : |
  # - name: shared-data
  #   mountPath: /mnt/shared-data
  #   readOnly: false

extraVolumeMounts: {}
# extraVolumeMounts: |
  # |
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

VolumeMountsConfig: {}
# VolumeMountsConfig: |
  # - name: shared-data
  #   emptyDir: {}
  # - name: extras
  #   configMap:
  #     name: config-template


statefulset:
  enabled: false
  ## Pods replace strategy
  ## ref: https://v1-10.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#statefulsetupdatestrategy-v1-apps
  # strategy: {}
  revisionHistoryLimit: 10
  terminationGracePeriodSeconds: 10
  # annotations:
  #   name: value
  # labels:
  #   name: value
  pod:
    # securityContext: {}
    # hostAliases:
    #  - ip: "10.1.2.3"
    #    hostnames:
    #    - "foo.remote"
    #    - "bar.remote"
    annotations: {}
    ## Read more about kube2iam to provide access to s3 https://github.com/jtblin/kube2iam
    #  iam.amazonaws.com/role: role-arn
    labels: {}
    # command:
    # args:
  ## Configure volumeClaimTemplate block
  persistence:
    enabled: true
    useVolumeClaimTemplates: true
    accessMode: ReadWriteOnce
    size: 8Gi
    mountPath: /data
    # storageClass: gp2
  #   annotations:
  #     name: value
  #   labels:
  #     name: value

daemonset:
  enabled: false
  ## Pods replace strategy
  ## ref: https://v1-10.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#daemonsetupdatestrategy-v1-apps
  # strategy: {}
  revisionHistoryLimit: 10
  # annotations:
  #   name: value
  # labels:
  #   name: value
  pod:
    # securityContext: {}
    # hostAliases:
    #  - ip: "10.1.2.3"
    #    hostnames:
    #    - "foo.remote"
    #    - "bar.remote"
    annotations: {}
    ## https://github.com/uswitch/kiam
    ## https://github.com/jtblin/kube2iam
    #  iam.amazonaws.com/role: role-arn
    labels: {}
    # command:
    args: []

job:
  default:
    enabled: false
    # labels:
    #   name: value
    # annotations:
    #   name: value
    restartPolicy: Never
    pod:
      # securityContext: {}
      # hostAliases:
      #  - ip: "10.1.2.3"
      #    hostnames:
      #    - "foo.remote"
      #    - "bar.remote"
      annotations: {}
      ## https://github.com/uswitch/kiam
      ## https://github.com/jtblin/kube2iam
      #  iam.amazonaws.com/role: role-arn
      labels: {}
      # command:
      args: []

cronjob:
  default:
    enabled: false
    # labels:
    #   name: value
    # annotations:
    #   name: value
    successfulJobsHistoryLimit: 1
    failedJobsHistoryLimit: 1
    concurrencyPolicy: Forbid
    schedule: "* * */15 * *"
    activeDeadlineSeconds: 300
    startingDeadlineSeconds: 180
    restartPolicy: Never
    pod:
      # securityContext: {}
      # hostAliases:
      #  - ip: "10.1.2.3"
      #    hostnames:
      #    - "foo.remote"
      #    - "bar.remote"
      annotations: {}
      ## https://github.com/uswitch/kiam
      ## https://github.com/jtblin/kube2iam
      #  iam.amazonaws.com/role: role-arn
      labels: {}
      # command:
      args: []

service:
  enabled: false
  type: ClusterIP
  # ports:
  #   name:
  #     internal: 80
  #     external: 80
  # labels:
  #   name: value
  # annotations:
  #   name: value

## ServiceMonitor CRDs to create & be scraped by the Prometheus instance.
## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/service-monitor.md
## Ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#servicemonitorspec
##
serviceMonitors: {}
#  ## Name of the ServiceMonitor to create
#  ##
#  name:
#
#    ## Labels to set used for the ServiceMonitorSelector.
#    labels:
#      prometheus: kube-prometheus
#
#   ## Service label for use in assembling a job name of the form <label value>-<port>
#   ## If no label is specified, the service name is used.
#   jobLabel: ""
#
#    ## Label selector for services to which this ServiceMonitor applies
#    ##
#    selector:
#      matchLabels:
#        app: nginx-ingress
#
#
#    ## Namespaces from which services are selected
#    ##
#    namespaceSelector:
#      ## Match any namespace
#      ##
#      # any: false
#
#      ## Explicit list of namespace names to select
#      ##
#      # matchNames: []
#
#
#    ## Endpoints of the selected service to be monitored
#    ##
#    endpoints:
#      ## Name of the endpoint's service port
#      ## Mutually exclusive with targetPort
#      # - port: ""
#
#      ## Name or number of the endpoint's target port
#      ## Mutually exclusive with port
#      # - targetPort: ""
#
#      ## File containing bearer token to be used when scraping targets
#      ##
#      #   bearerTokenFile: ""
#
#      ## Interval at which metrics should be scraped
#      ##
#      #   interval: 30s
#
#      ## HTTP path to scrape for metrics
#      ##
#      #   path: /metrics
#
#      ## HTTP scheme to use for scraping
#      ##
#      #   scheme: http
#
#      ## TLS configuration to use when scraping the endpoint
#      ##
#      #   tlsConfig:
#
#          ## Path to the CA file
#          ##
#          # caFile: ""
#
#          ## Path to client certificate file
#          ##
#          # certFile: ""
#
#         ## Skip certificate verification
#          ##
#          # insecureSkipVerify: false
#
#          ## Path to client key file
#          ##
#          # keyFile: ""
#
#          ## Server name used to verify host name
#          ##
#          # serverName: ""

## https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusrulespec
prometheusRules: {}
#  name:
#    labels:
#      prometheus: kube-prometheus
#    groups:
#    - name: prometheus.rules
#      rules:
#      - alert: PrometheusConfigReloadFailed
#        expr: prometheus_config_last_reload_successful == 0
#        for: 10m
#        labels:
#          severity: warning
#        annotations:
#          description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
#          summary: Reloading Promehteus' configuration failed

probes: {}
#  livenessProbe:
#    httpGet:
#      path: /
#      port: http
#  readinessProbe:
#    httpGet:
#      path: /
#      port: http

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources:
  requests:
    memory: 256Mi
    cpu: 100m
#  limits:
#    cpu: 100m
#    memory: 128Mi

## Init container resources defaults
initContainer:
  resources:
    requests:
      memory: 10Mi
      cpu: 10m

persistence:
  enabled: false
  storageName: storage
  mountPath: /data
  accessMode: ReadWriteOnce
  size: 8Gi
  # annotations:
  #   name: value
  # labels:
  #   name: value

  ## A manually managed Persistent Volume and Claim
  ## Requires Persistence.Enabled: true
  ## If defined, PVC must be created manually before volume will be bound
  # existingClaim:

  ## Data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ## set, choosing the default provisioner.  (gp2 on AWS, standard on
  ## GKE, AWS & OpenStack)
  ##
  # storageClass: "-"

# Ingress for load balancer
ingress:
  default:
    enabled: false
#    port: port-name
#    labels:
#      dns: "route53"
#    annotations:
#      kubernetes.io/ingress.class: nginx
#      kubernetes.io/tls-acme: "true"
#    hosts:
#      "domain.com": /
#      "www.domain.com": /
#    tls:
#    - secretName: server-tls
#      hosts:
#      - domain.com

# Istio Virtual Services for load balancer
virtualServices:
  default:
    enabled: false
#    hosts:
#      - "domain.com"
#    http:
#    - name: "reviews-v2-routes"
#      match:
#      - uri:
#          prefix: "/wpcatalog"
#      - uri:
#          prefix: "/consumercatalog"
#      rewrite:
#        uri: "/newcatalog"
#      route:
#      - destination:
#          host: reviews.prod.svc.cluster.local
#          subset: v2
#    - name: "reviews-v1-route"
#      route:
#      - destination:
#          host: reviews.prod.svc.cluster.local
#          subset: v1


crd: {}
#  "networking.istio.io/v1alpha3":
#    ServiceEntry:
#      default:
#        enabled: true
#        spec:
#          hosts:
#            - www.googleapis.com
#            ports:
#              - number: 443
#                name: https
#                protocol: HTTPS
#            location: MESH_EXTERNAL
#            resolution: DNS


## Node selector
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

## Affinity
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

## Tolerations for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## Security context
securityContext:
  enabled: false
  runAsUser: 999
  fsGroup: 999


# Helm charts don't allow include a file outside from the chart directory.
# To workaround this, we can use the "set-file" parameters to include a file inside
# of a Values and after this, put this Value inside an ConfigMapfile (cmf)
# You need to follow this exact syntax in your helmfile
# set:
#   - name: cmf.The_Name_of_the_Resource.data
#     file: /path/relative/to/manifest
#
# NOTE: Rememeber to add the cmf.name to "envFrom.configMaps" to have it inside of container.

## Security context
IncludeDataDogEnv:
  enabled: false

IncludeForecastleEnv:
  enabled: false
  group: "Unlimited"
  instance: "oidc-ingress"


# Networkpolices https://kubernetes.io/docs/concepts/services-networking/network-policies/
# Spoton documentation https://github.com/vhs-spoton/personal-doc/blob/main/networkpolices.md
# If enable, will allow or deny input or output traffic to specific port/pod/namespace/cidr.
networkPolicy:
  enabled: false

  annotations: []

  # This will disable all the input/output traffic by default, except the DNS queries, TCP/443 and KIAM traffic (TCP/80 for 169.254.169.254).
  # Is recommendable to have it disabled at begin to make tests, but enable it on production.
  DenyALL:
    enabled: true

  # The input traffic rules. Each pod can have many input rules or a combination of they in one rule. Some examples:
  # One rule with many arguments:
  # Allow all traffic from namespace staging and prod on the port 5432
  # Allow all traffic from namespace staging and prod and where the pod label is "API" on the port 5432 and 443
  # Two rules with similiar arguments:
  # Allow all traffic from everyone on port 443.
  # Allow all traffic from 8.8.8.8 on port 80 and 21
  # We can use 3 types of Selector ipBlock, namespaceSelector and podSelector, plus the ports (port and protocol)
  # VERY IMPORTANT... using more than one selector will work as "OR" not like as "AND" in this template.

  # In the example above, we are allowing the input traffic from the IPs 8.8.8.8 OR 1.1.1.1 OR from namespace: staging 
  # OR namespace: prod OR pods with the role "frontend" on the ports 5432/TCP and 123/UDP
  Ingress: {}
  #   - name: test-ingress
  #     selector:
  #       ipBlock:
  #         - 8.8.8.8
  #         - 1.1.1.1
  #       namespaceSelector:
  #         namespace: stating
  #         namespace: prod
  #       podSelector:
  #         role: frontend
  #     ports:
  #       - port: 5432
  #         protocol: TCP
  #       - port: 123
  #         protocol: UDP

  #   # In this case, we are allowing only access to the port 443/TCP to all the pods with the role "api"
  #   - name: api-spoton
  #     selector:
  #       podSelector:
  #         role: api

  #     ports:
  #       - port: 443
  #         protocol: TCP

  # # The EGRESS rules will filter all the output traffic and is than necesary like the ingress.
  Egress: {}
  #   # In this example we are allowing traffic HTTP and HTTPS to all destinationes (not very good, but is ok)
  #   - name: http-and-https
  #     ports:
  #       - port: 80
  #         protocol: TCP
  #       - port: 443
  #         protocol: UDP

  #   # The same rule, but restricted only to traffic going to the same namespace and to port 443.
  #   - name: http-and-https
  #     selector:
  #       namespaceSelector:
  #         namespace: staging
  #     ports:
  #       - port: 80
  #         protocol: TCP
  #       - port: 443
  #         protocol: UDP

  #   # here a very good rule that only allow access to the databases to the pods with role "database" inside of the namespace
  #   - name: only-database
  #     selector:
  #       namespaceSelector:
  #         namespace: restaurant
  #       podSelector:
  #         role: databases
  #     ports:
  #       - port: 5432
  #         protocol: TCP


